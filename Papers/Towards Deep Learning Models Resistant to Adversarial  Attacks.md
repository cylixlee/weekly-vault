# 迈向对抗样本鲁棒的深度学习模型

<small style="color: gray; font-family: 'IBM Plex Sans SC Medium'">2025/02/03 · 周论文阅读 · 李政翱</small>

> **来源**
> 
> 本文是经典的对抗样本相关论文，提出了*对抗训练* 方法。原文为 [Towards Deep Learning Models Resistant to Adversarial Attacks](https://openreview.net/forum?id=rJzIBfZAb)，收录于 ICLR 2018。

本文使用自然鞍点（min-max）公式描述对抗攻击中的安全概念。这一公式使我们能够将攻击和防御转换到一个共同的理论框架中，自然地封装了大多数之前关于对抗样本的工作。本文的主要贡献如下：

1. 本文**对鞍点公式对应的优化场景进行了仔细研究**。虽然该问题的组成部分具有非凸非凹性，但该问题是可处理的。此外，还得出 PGD 方法是利用了网络局部一阶信息的最强攻击方法。
2. 探讨了**网络架构对于对抗性鲁棒性的影响**，并发现模型容量在这里起着重要的作用。为了可靠地抵御强大的对抗攻击，网络需要比只正确分类良性例子更大的容量。
3. 基于上述见解，本文在 MNIST 和 CIFAR10 数据集上**训练网络**，这些网络对广泛的对抗性攻击具有鲁棒性。

总的来说，这些发现表明，安全的神经网络是触手可及的。

## 对抗鲁棒性的优化视角

### 经验风险最小化（ERM）框架

考虑一个标准分类任务：

- 数据分布 $\mathcal D$ 由多对输入 $x \in \mathbb R^d$ 及其标签 $y \in [k]$ 组成；
- 假设给定了一个恰当的损失函数 $L(\theta,x,y)$ ，比如交叉熵损失；
- 模型参数使用 $\theta \in \mathbb R^p$ 表示。

模型的训练目标即为寻找模型参数 $\theta$ 使得**期望风险** $\mathbb E_{(x,y) \sim D}\left[L(x,y,\theta)\right]$ 最小。

> **期望风险**
> 
> 期望风险的核心含义是模型参数 $\theta$ 在自然数据分布 $\mathcal D$ 上的平均损失，也即模型在未受干扰的干净数据上的性能表现。从*优化* 的角度来看，模型在训练时并不直接根据“正确率”来调整参数，而是根据损失来修正误差。

以上的定义称作**经验风险最小化**框架（Empirical risk minimization, ERM）。这是机器学习中的一种核心优化框架。然而，这种经典的框架并不能产生对抗鲁棒的模型。形式上来说，存在很多对抗攻击方法以属于 $c_1$ 类别的样本 $x$ 作为输入，并找到样本 $x^{adv}$，使得 $x^{adv}$ 与 $x$ 相当接近，但让模型误分类为 $c_2 \ne c_1$ 类别。

### 增强的 ERM 框架

为了可靠地训练对抗鲁棒的模型，需要对传统的 ERM 模型进行增强。首先需要定义对抗攻击：对于每个数据点 $x$，引入一系列允许的扰动 $\mathcal S \subseteq \mathbb R^d$ 作为对抗攻击的形式化操作能力。

接下来，使用以上定义的攻击来更改原有的 ERM 公式：
$$
\min_{\theta} \rho(\theta), \quad \text{where}\ \rho(\theta)=\mathbb E_{(x,y)\sim\mathcal D}\left[\max_{\delta\in\mathcal S}L(\theta,x+\delta,y)\right]
$$
作为本文主要研究的公式。这个公式给予了一个统一的视角，包含了许多之前关于对抗鲁棒性的工作成果。本文的观点源于将鞍点问题看作是一个内部最大化问题和一个外部最小化问题的组成部分。这两个问题在我们的语境中都有很自然的解释：

- **内部最大化**问题的目的是找到一个给定的数据点 $x$ 的对抗版本 $x^{adv}$ 。这正是攻击一个给定的神经网络的问题。
- **外部最小化**问题的目标是找到模型参数，使得内攻击问题给出的“对抗损失”最小化。

这正是使用对抗训练技术来训练鲁棒分类器的问题。

## 迈向普遍鲁棒的网络

根据增强的 ERM 公式，对抗损失 $\rho(\theta)$ 足够小时保证允许的攻击方法无法欺骗网络。因此，如何求得上述公式的解是本文的主要关注点。

不幸的是，我们无法知道合理时间内是否能求得该方程的良好的解。解决鞍点问题涉及到解决**非凸外最小化问题**和**非凹内最大化问题**。本文的关键贡献之一是证明了，在实践中，人们确实可以解决鞍点问题。

![[Adversarial Training.png]]

虽然在 $x_i+\mathcal S$ 中有许多局部极大值，但它们往往有非常集中的损失值。这与朴素的观点相呼应，即训练神经网络是可能的，因为损失通常有许多具有非常相似的局部极小值。

## 网络容量与对抗鲁棒性

对于可能的扰动的固定集合 $\mathcal S$，问题的值完全依赖于我们正在学习的分类器的体系结构。因此，该模型的体系结构容量成为影响其整体性能的一个主要因素。在高层次上，以鲁棒的方式对例子进行分类需要更强的分类器，因为**对抗样本的存在将问题的决策边界改变为更复杂的决策边界**。

![[Adversarial Decision Boundary.png]]
观察到以下现象：

- **仅增大容量就能增加鲁棒性**。我们观察到，当只使用自然样本进行训练（除了提高这些例子的准确性）时，增加了网络的容量就增加了对一步扰动的鲁棒性。当遇到 $\epsilon$ 较小的对抗样本时，这种效果更大。
- **对于较大的 $\varepsilon$，FGSM 对抗训练不会增加鲁棒性**。当使用由 FGSM 生成的对抗样本来训练网络时，我们观察到网络对这些对抗样本过拟合。这种行为被称为*标签泄漏*，它源于这样一个事实，即对手产生了一组非常有限的对抗样本，网络可以过度适应。这些网络在自然样本上的性能较差，并且对 PGD 对手没有表现出任何形式的鲁棒性。对于较小的 $\varepsilon$ 的情况，在自然例子的 $l_\infty$ 球中，损失是足够线性的，FGSM 发现接近 PGD 发现的对抗例子，因此是一个合理的对手来训练。
- **弱模型可能无法学习较复杂的分类器**。在小容量网络的情况下，试图对抗训练一个强大的对手（PGD）会阻止网络学习任何有意义的东西。该网络收敛到总是预测一个固定的类，即使它可以通过标准的训练收敛到一个准确的分类器。*网络的小容量迫使训练过程牺牲在自然样本上的性能，以提供对抗鲁棒性*。
- **鞍点问题的值随着容量的增加而减少**。指定一个敌对模型，并对其进行训练，增强 ERM 框架公式的值随着容量的增加而下降，这表明该模型可以越来越好地拟合对抗样本。
- **更大的模型容量和更强大的对手降低了可迁移性**。无论是增加网络的容量，还是对内部优化问题使用一种更强的方法，都会降低迁移对抗攻击的有效性。我们通过观察来自源和传输网络的梯度之间的相关性，随着容量的增加而变得不那么显著，从而通过实验验证了这一点。

## 结论

本文的发现提供了证据，表明深度神经网络可以抵抗对抗攻击。正如本文的理论和实验所表明的那样，我们可以设计出可靠的对抗训练方法。这背后的一个关键见解是底层优化任务出乎意料的规则结构：尽管相关问题对应于一个具有许多不同局部极大值的高度非凹函数的最大化，但它们的值是高度集中的。总的来说，我们的发现给了我们希望，即对抗鲁棒的深度学习模型目前可能是触手可及的。

对于 MNIST 数据集，我们的网络非常健壮，在各种强大的 $l_\infty$ 绑定对手和大的扰动下实现了高精度。我们在 CIFAR10 上的实验尚未达到相同的性能水平。然而，我们的结果已经表明，我们的技术导致显著提高网络的鲁棒性。我们相信，进一步探索这一方向将在该数据集上实现对抗鲁棒网络。

